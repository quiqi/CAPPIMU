{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**DanHAR网络测试**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "train_x = np.load('/home/intelligence/Robin/Dataset/train_x.npy')\n",
    "train_x = torch.FloatTensor(train_x)\n",
    "train_x = train_x[0:64,:,:]\n",
    "train_x = train_x.unsqueeze(1)\n",
    "train_x.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "input = torch.randn(64,128,1,1)\n",
    "net = nn.Conv2d(128 , 8, 1, bias=False)\n",
    "net(input).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=(7,1)):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        assert kernel_size in ((3,1), (7,1)), 'kernel size must be 3 or 7'\n",
    "        padding = (3,0) if kernel_size == (7,1) else (1,0)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=128, kernel_size=(6, 1), stride=(3, 1), padding=(1, 0)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "h1 = Block1(train_x)  # [64,128,116,25]\n",
    "shortcut1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=128, kernel_size=(6, 1), stride=(3, 1), padding=(1, 0)),\n",
    "            nn.BatchNorm2d(128),\n",
    "        )\n",
    "r = shortcut1(train_x)  # [64, 128, 116, 25]\n",
    "\n",
    "ca1 = ChannelAttention(128)  # [64,128,1,1]\n",
    "sa1 = SpatialAttention()  # [64,1,116,25]\n",
    "h1 = ca1(h1) * h1  # [64,128,1,1] * [64, 128, 116, 25] = [64, 128, 116, 25]\n",
    "h1=  sa1(h1) * h1  # [64, 1, 116, 25] * [64,128,116,25] = [64,128, 116, 25]\n",
    "h1 = h1 + r  # [64, 128, 116, 25]\n",
    "\n",
    "Block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(6, 1), stride=(3, 1), padding=(1, 0)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "h2 = Block2(h1)  # [64, 256, 38, 25]\n",
    "\n",
    "shortcut2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(6, 1), stride=(3, 1), padding=(1, 0)),\n",
    "            nn.BatchNorm2d(256),\n",
    "        )\n",
    "r = shortcut2(h1)  # [64, 256, 38, 25]\n",
    "\n",
    "ca2 = ChannelAttention(256)  # [64,256,1,1]\n",
    "sa2 = SpatialAttention()  # [64,1,38,25]\n",
    "h2 = ca2(h2) * h2  # [64,256,1,1] * [64, 256, 38, 25] = [64, 256, 38, 25]\n",
    "h2 = sa2(h2) * h2  # [64,1,38,25] * [64, 256, 38, 25] = [64, 256, 38, 25]\n",
    "h2 = h2 + r  # [64, 256, 38, 25]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Block3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=384, kernel_size=(6, 1), stride=(3, 1), padding=(1, 0)),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=384, out_channels=384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0)),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "shortcut3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=384, kernel_size=(6, 1), stride=(3, 1), padding=(1, 0)),\n",
    "            nn.BatchNorm2d(384),\n",
    "        )\n",
    "\n",
    "h3 = Block3(h2)  # [64, 384, 12, 25]\n",
    "r = shortcut3(h2)  # [64, 384, 12, 25]\n",
    "ca3 = ChannelAttention(384)  # [64, 384, 1, 1]\n",
    "sa3 = SpatialAttention()  # [64, 1, 12, 25]\n",
    "h3 = ca3(h3) * h3  # [64, 384, 12, 25]\n",
    "h3 = sa3(h3) * h3  # [64, 384, 12, 25]\n",
    "h3 = h3 + r  # [64, 384, 12, 25]\n",
    "\n",
    "x = h3.view(h3.size(0), -1)  # [64,115200]\n",
    "x.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TAMA测试**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "\n",
    "\n",
    "class SimplifiedScaledDotProductAttention(nn.Module):\n",
    "    '''\n",
    "    Scaled dot-product attention\n",
    "    '''\n",
    "\n",
    "    def __init__(self, d_model, h,dropout=.1):\n",
    "        '''\n",
    "        :param d_model: Output dimensionality of the model\n",
    "        :param d_k: Dimensionality of queries and keys\n",
    "        :param d_v: Dimensionality of values\n",
    "        :param h: Number of heads\n",
    "        '''\n",
    "        super(SimplifiedScaledDotProductAttention, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model//h\n",
    "        self.d_v = d_model//h\n",
    "        self.h = h\n",
    "\n",
    "        self.fc_o = nn.Linear(h * self.d_v, d_model)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, queries, keys, values, attention_mask=None, attention_weights=None):\n",
    "        '''\n",
    "        Computes\n",
    "        :param queries: Queries (b_s, nq, d_model)\n",
    "        :param keys: Keys (b_s, nk, d_model)\n",
    "        :param values: Values (b_s, nk, d_model)\n",
    "        :param attention_mask: Mask over attention values (b_s, h, nq, nk). True indicates masking.\n",
    "        :param attention_weights: Multiplicative weights for attention values (b_s, h, nq, nk).\n",
    "        :return:\n",
    "        '''\n",
    "        b_s, nq = queries.shape[:2]\n",
    "        nk = keys.shape[1]\n",
    "\n",
    "        q = queries.view(b_s, nq, self.h, self.d_k).permute(0, 2, 1, 3)  # (b_s, h, nq, d_k)\n",
    "        k = keys.view(b_s, nk, self.h, self.d_k).permute(0, 2, 3, 1)  # (b_s, h, d_k, nk)\n",
    "        v = values.view(b_s, nk, self.h, self.d_v).permute(0, 2, 1, 3)  # (b_s, h, nk, d_v)\n",
    "\n",
    "        att = torch.matmul(q, k) / np.sqrt(self.d_k)  # (b_s, h, nq, nk)\n",
    "        if attention_weights is not None:\n",
    "            att = att * attention_weights\n",
    "        if attention_mask is not None:\n",
    "            att = att.masked_fill(attention_mask, -np.inf)\n",
    "        att = torch.softmax(att, -1)\n",
    "        att=self.dropout(att)\n",
    "\n",
    "        out = torch.matmul(att, v).permute(0, 2, 1, 3).contiguous().view(b_s, nq, self.h * self.d_v)  # (b_s, nq, h*d_v)\n",
    "        out = self.fc_o(out)  # (b_s, nq, d_model)\n",
    "        return out\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    '''\n",
    "    Scaled dot-product attention\n",
    "    '''\n",
    "\n",
    "    def __init__(self, d_model, d_k, d_v, h,dropout=.1):\n",
    "        '''\n",
    "        :param d_model: Output dimensionality of the model\n",
    "        :param d_k: Dimensionality of queries and keys\n",
    "        :param d_v: Dimensionality of values\n",
    "        :param h: Number of heads\n",
    "        '''\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.fc_q = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_k = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_v = nn.Linear(d_model, h * d_v)\n",
    "        self.fc_o = nn.Linear(h * d_v, d_model)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.h = h\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, queries, keys, values, attention_mask=None, attention_weights=None):\n",
    "        '''\n",
    "        Computes\n",
    "        :param queries: Queries (b_s, nq, d_model)\n",
    "        :param keys: Keys (b_s, nk, d_model)\n",
    "        :param values: Values (b_s, nk, d_model)\n",
    "        :param attention_mask: Mask over attention values (b_s, h, nq, nk). True indicates masking.\n",
    "        :param attention_weights: Multiplicative weights for attention values (b_s, h, nq, nk).\n",
    "        :return:\n",
    "        '''\n",
    "        b_s, nq = queries.shape[:2]\n",
    "        nk = keys.shape[1]\n",
    "\n",
    "        q = self.fc_q(queries).view(b_s, nq, self.h, self.d_k).permute(0, 2, 1, 3)  # (b_s, h, nq, d_k)\n",
    "        k = self.fc_k(keys).view(b_s, nk, self.h, self.d_k).permute(0, 2, 3, 1)  # (b_s, h, d_k, nk)\n",
    "        v = self.fc_v(values).view(b_s, nk, self.h, self.d_v).permute(0, 2, 1, 3)  # (b_s, h, nk, d_v)\n",
    "\n",
    "        att = torch.matmul(q, k) / np.sqrt(self.d_k)  # (b_s, h, nq, nk)\n",
    "        if attention_weights is not None:\n",
    "            att = att * attention_weights\n",
    "        if attention_mask is not None:\n",
    "            att = att.masked_fill(attention_mask, -np.inf)\n",
    "        att = torch.softmax(att, -1)\n",
    "        att=self.dropout(att)\n",
    "\n",
    "        out = torch.matmul(att, v).permute(0, 2, 1, 3).contiguous().view(b_s, nq, self.h * self.d_v)  # (b_s, nq, h*d_v)\n",
    "        out = self.fc_o(out)  # (b_s, nq, d_model)\n",
    "        return out\n",
    "\n",
    "class PositionAttentionModule(nn.Module):\n",
    "\n",
    "    def __init__(self,d_model=512,kernel_size=3,H=7,W=7):\n",
    "        super().__init__()\n",
    "        self.cnn=nn.Conv2d(d_model,d_model,kernel_size=kernel_size,padding=(kernel_size-1)//2)\n",
    "        self.pa=ScaledDotProductAttention(d_model,d_k=d_model,d_v=d_model,h=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        bs,c,h,w=x.shape\n",
    "        y=self.cnn(x)\n",
    "        y=y.view(bs,c,-1).permute(0,2,1) #bs,h*w,c\n",
    "        y=self.pa(y,y,y) #bs,h*w,c\n",
    "        return y\n",
    "\n",
    "\n",
    "class ChannelAttentionModule(nn.Module):\n",
    "\n",
    "    def __init__(self,d_model=512,kernel_size=3,H=7,W=7):\n",
    "        super().__init__()\n",
    "        self.cnn=nn.Conv2d(d_model,d_model,kernel_size=kernel_size,padding=(kernel_size-1)//2)\n",
    "        self.pa=SimplifiedScaledDotProductAttention(H*W,h=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        bs,c,h,w=x.shape\n",
    "        y=self.cnn(x)\n",
    "        y=y.view(bs,c,-1) #bs,c,h*w\n",
    "        y=self.pa(y,y,y) #bs,c,h*w\n",
    "        return y\n",
    "\n",
    "\n",
    "class DAModule(nn.Module):\n",
    "\n",
    "    def __init__(self,d_model=512,kernel_size=3,H=7,W=7):\n",
    "        super().__init__()\n",
    "        self.position_attention_module=PositionAttentionModule(d_model=512,kernel_size=3,H=7,W=7)\n",
    "        self.channel_attention_module=ChannelAttentionModule(d_model=512,kernel_size=3,H=7,W=7)\n",
    "\n",
    "    def forward(self,input):\n",
    "        bs,c,h,w=input.shape\n",
    "        p_out=self.position_attention_module(input)\n",
    "        c_out=self.channel_attention_module(input)\n",
    "        p_out=p_out.permute(0,2,1).view(bs,c,h,w)\n",
    "        c_out=c_out.view(bs,c,h,w)\n",
    "        return p_out+c_out\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input=torch.randn(50,512,7,7)\n",
    "    danet=DAModule(d_model=512,kernel_size=3,H=7,W=7)\n",
    "    print(danet(input).shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([128, 128, 9])"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input = torch.rand(128, 128, 9)\n",
    "a = torch.rand(128, 128, 1)\n",
    "(input * a).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}