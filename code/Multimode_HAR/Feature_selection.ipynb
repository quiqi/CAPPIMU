{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape:(27819, 300, 16)\n",
      "train_y shape:(27819,)\n",
      "test_x shape:(11923, 300, 16)\n",
      "test_y shape:(11923,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_x = np.load('/home/intelligence/Robin/Dataset/save_raw_data/insole_train_win_x.npy')\n",
    "train_y = np.load('/home/intelligence/Robin/Dataset/save_raw_data/insole_train_win_y.npy')\n",
    "test_x = np.load('/home/intelligence/Robin/Dataset/save_raw_data/insole_test_win_x.npy')\n",
    "test_y = np.load('/home/intelligence/Robin/Dataset/save_raw_data/insole_test_win_y.npy')\n",
    "print(f'train_x shape:{train_x.shape}')\n",
    "print(f'train_y shape:{train_y.shape}')\n",
    "print(f'test_x shape:{test_x.shape}')\n",
    "print(f'test_y shape:{test_y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27819, 300, 16)\n"
     ]
    }
   ],
   "source": [
    "# 分别提取出左右脚数据\n",
    "left_train_x = train_x[:,:,0:8]\n",
    "right_train_x = train_x[:,:,8:16]\n",
    "left_test_x = test_x[:,:,0:8]\n",
    "right_test_x = test_x[:,:,8:16]\n",
    "\n",
    "train_x_1 = left_train_x[:,:,0:5]\n",
    "train_x_2 = left_train_x[:,:,5:8]\n",
    "train_x_3 = right_train_x[:,:,0:5]\n",
    "train_x_4 = right_train_x[:,:,5:8]\n",
    "\n",
    "test_x_1 = left_test_x[:,:,0:5]\n",
    "test_x_2 = left_test_x[:,:,5:8]\n",
    "test_x_3 = right_test_x[:,:,0:5]\n",
    "test_x_4 = right_test_x[:,:,5:8]\n",
    "\n",
    "train_x_new = np.concatenate((train_x_1, train_x_2, train_x_3, train_x_4),axis=2)\n",
    "test_x_new = np.concatenate((test_x_1, test_x_2, test_x_3, test_x_4),axis=2)\n",
    "print(train_x_new.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, n_channels=16, n_timesteps=300, n_classes=21, out_channels=128):\n",
    "        super(FCN, self).__init__()\n",
    "\n",
    "        self.conv_block1 = nn.Sequential(nn.Conv1d(n_channels, 32, kernel_size=8, stride=1, bias=False, padding=4),\n",
    "                                         nn.BatchNorm1d(32),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.MaxPool1d(kernel_size=2, stride=2, padding=1),\n",
    "                                         nn.Dropout(0.35))\n",
    "        self.conv_block2 = nn.Sequential(nn.Conv1d(32, 64, kernel_size=8, stride=1, bias=False, padding=4),\n",
    "                                         nn.BatchNorm1d(64),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.MaxPool1d(kernel_size=2, stride=2, padding=1))\n",
    "        self.conv_block3 = nn.Sequential(nn.Conv1d(64, out_channels, kernel_size=8, stride=1, bias=False, padding=4),\n",
    "                                         nn.BatchNorm1d(out_channels),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.MaxPool1d(kernel_size=2, stride=2, padding=1))\n",
    "\n",
    "        self.out_len = n_timesteps\n",
    "        for _ in range(3):\n",
    "            self.out_len = (self.out_len + 1) // 2 + 1\n",
    "\n",
    "        self.out_channels = out_channels  # 128\n",
    "        self.out_dim = self.out_len * self.out_channels\n",
    "\n",
    "        self.logits = nn.Linear(self.out_len * out_channels, n_classes)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        x_in = x_in.permute(0, 2, 1)\n",
    "        x = self.conv_block1(x_in)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "\n",
    "        x_flat = x.reshape(x.shape[0], -1)\n",
    "        logits = self.logits(x_flat)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from torch.utils import data\n",
    "from copy import deepcopy\n",
    "import sklearn.metrics\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"构造一个pytorch数据构造器\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "def train(net, device, train_loader, test_loader, loss, num_epochs, optimizer, scheduler, metrics):\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "\n",
    "    # 声明需要保存的量\n",
    "    train_loss, test_loss, train_metrics, test_metrics, time_list = [], [], [], [], []\n",
    "    max_test_metrics = 0.0\n",
    "    # animator = d2l.Animator(xlabel='epoch', xlim=[1,num_epochs], ylim=[0,1],legend=['train loss', 'test loss', 'test accuracy'])\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss_batch, test_loss_batch, train_metrics_batch, test_metrics_batch = [], [], [], []\n",
    "        start = time.time()\n",
    "        # 训练\n",
    "        if isinstance(net, nn.Module):\n",
    "            net.train()  # 设置成训练模式，要计算梯度\n",
    "        for X, y in train_loader:\n",
    "            # 获取训练数据和标签\n",
    "            X, y = X.to(device).float(), y.to(device).long()\n",
    "            # 数据输入网络，前向传播\n",
    "            y_hat = net(X)\n",
    "            # 网络输出和标签输入损失函数，求得损失\n",
    "            l = loss(y_hat, y)\n",
    "            if isinstance(optimizer, torch.optim.Optimizer):\n",
    "                optimizer.zero_grad()  # 清零优化器，梯度清零\n",
    "                l.backward()  # 损失反向传播，计算梯度\n",
    "                optimizer.step()  # 更新优化器，更新参数\n",
    "            # 累加损失\n",
    "            train_loss_batch.append(l.item())\n",
    "            train_metrics_batch.append(metrics(y.cpu(), torch.argmax(y_hat, dim=1).cpu(), average='micro'))\n",
    "        scheduler.step()\n",
    "        # 累加训练损失\n",
    "        train_loss.append(np.mean(train_loss_batch))\n",
    "        train_metrics.append(np.mean(train_metrics_batch))\n",
    "        time_list.append(time.time() - start)\n",
    "        # 验证\n",
    "        if isinstance(net, nn.Module):  # 如果是用torch.nn实现的模型，就将它转换成评估模式\n",
    "            net.eval()  # 将模型设置为评估模式：不再计算梯度，只做forward pass\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y.long())\n",
    "            test_loss_batch.append(l.item())\n",
    "            test_metrics_batch.append(metrics(y.cpu(), torch.argmax(y_hat, dim=1).cpu(), average='micro'))\n",
    "        # 累加验证损失\n",
    "        test_loss.append(np.mean(test_loss_batch))\n",
    "        test_metrics.append(np.mean(test_metrics_batch))\n",
    "        # 打印信息\n",
    "        print(\"Time:{:.3f}s...\".format(time_list[-1]),\n",
    "              \"Epoch:{}/{}...\".format(epoch + 1, num_epochs),\n",
    "              \"Train Loss:{:.4f}...\".format(train_loss[-1]),\n",
    "              \"Train Metrics:{:.4f}...\".format(train_metrics[-1]),\n",
    "              \"Val Loss:{:.4f}...\".format(test_loss[-1]),\n",
    "              \"Val Metrics:{:.4f}...\".format(test_metrics[-1]),\n",
    "              \"Lr:{:.5f}...\".format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "        if max_test_metrics < test_metrics[-1]:\n",
    "            max_test_metrics = test_metrics[-1]\n",
    "            best_model = deepcopy(net.state_dict())  # 保存训练epoch中指标最好的参数模型\n",
    "\n",
    "    print(\"max_test_metrics:{:.4f}...\".format(max_test_metrics))\n",
    "    return train_loss, test_loss, train_metrics, test_metrics, best_model\n",
    "\n",
    "def evaluate(test_loader, best_model, DEVICE, criterion):\n",
    "    model = FCN()\n",
    "    model = model.to(DEVICE)\n",
    "    model.load_state_dict(best_model)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = []\n",
    "        test_metrics_batch = []\n",
    "        for idx, (sample, target) in enumerate(test_loader):\n",
    "            sample, target = sample.to(DEVICE), target.to(DEVICE).long()\n",
    "            out = model(sample)\n",
    "            loss = criterion(out, target)\n",
    "            test_loss.append(loss.item())\n",
    "            test_metrics_batch.append(sklearn.metrics.f1_score(target.cpu(), torch.argmax(out, dim=1).cpu(),\n",
    "                                                               average='micro'))\n",
    "            outputs = torch.argmax(out, dim=1)\n",
    "\n",
    "        test_loss_avg = np.mean(test_loss)\n",
    "        test_metrics_avg = np.mean(test_metrics_batch)\n",
    "        test_f1_score = float(test_metrics_avg) * 100.0\n",
    "\n",
    "    print(f'Evaluate:\\n')\n",
    "    print(f'Final Test Loss : {test_loss_avg:.4f}\\t | \\tFinal Test F1_Score : {test_f1_score:2.4f}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in GPU\n",
      "training on cuda\n",
      "Time:0.931s... Epoch:1/100... Train Loss:3.2320... Train Metrics:0.2768... Val Loss:1.5305... Val Metrics:0.4513... Lr:0.00850...\n",
      "Time:0.923s... Epoch:2/100... Train Loss:1.3780... Train Metrics:0.5114... Val Loss:1.1746... Val Metrics:0.5790... Lr:0.00723...\n",
      "Time:0.907s... Epoch:3/100... Train Loss:1.1313... Train Metrics:0.5973... Val Loss:1.0665... Val Metrics:0.6049... Lr:0.00614...\n",
      "Time:1.126s... Epoch:4/100... Train Loss:0.9968... Train Metrics:0.6503... Val Loss:1.0280... Val Metrics:0.6298... Lr:0.00522...\n",
      "Time:0.938s... Epoch:5/100... Train Loss:0.8894... Train Metrics:0.6865... Val Loss:0.9296... Val Metrics:0.6702... Lr:0.00444...\n",
      "Time:0.941s... Epoch:6/100... Train Loss:0.8175... Train Metrics:0.7108... Val Loss:0.8743... Val Metrics:0.6834... Lr:0.00377...\n",
      "Time:1.024s... Epoch:7/100... Train Loss:0.7590... Train Metrics:0.7290... Val Loss:0.8131... Val Metrics:0.7203... Lr:0.00321...\n",
      "Time:0.939s... Epoch:8/100... Train Loss:0.7116... Train Metrics:0.7483... Val Loss:0.7786... Val Metrics:0.7340... Lr:0.00272...\n",
      "Time:0.940s... Epoch:9/100... Train Loss:0.6630... Train Metrics:0.7649... Val Loss:0.7395... Val Metrics:0.7433... Lr:0.00232...\n",
      "Time:0.934s... Epoch:10/100... Train Loss:0.6183... Train Metrics:0.7816... Val Loss:0.7055... Val Metrics:0.7611... Lr:0.00197...\n",
      "Time:1.130s... Epoch:11/100... Train Loss:0.5824... Train Metrics:0.7944... Val Loss:0.6552... Val Metrics:0.7736... Lr:0.00167...\n",
      "Time:0.930s... Epoch:12/100... Train Loss:0.5491... Train Metrics:0.8061... Val Loss:0.6427... Val Metrics:0.7799... Lr:0.00142...\n",
      "Time:0.927s... Epoch:13/100... Train Loss:0.5164... Train Metrics:0.8172... Val Loss:0.6067... Val Metrics:0.7952... Lr:0.00121...\n",
      "Time:1.065s... Epoch:14/100... Train Loss:0.4910... Train Metrics:0.8287... Val Loss:0.5918... Val Metrics:0.7997... Lr:0.00103...\n",
      "Time:0.898s... Epoch:15/100... Train Loss:0.4624... Train Metrics:0.8385... Val Loss:0.5931... Val Metrics:0.8032... Lr:0.00087...\n",
      "Time:0.913s... Epoch:16/100... Train Loss:0.4389... Train Metrics:0.8457... Val Loss:0.5713... Val Metrics:0.8109... Lr:0.00074...\n",
      "Time:0.901s... Epoch:17/100... Train Loss:0.4234... Train Metrics:0.8528... Val Loss:0.5587... Val Metrics:0.8119... Lr:0.00063...\n",
      "Time:1.114s... Epoch:18/100... Train Loss:0.4053... Train Metrics:0.8597... Val Loss:0.5434... Val Metrics:0.8162... Lr:0.00054...\n",
      "Time:0.903s... Epoch:19/100... Train Loss:0.3909... Train Metrics:0.8638... Val Loss:0.5188... Val Metrics:0.8231... Lr:0.00046...\n",
      "Time:0.910s... Epoch:20/100... Train Loss:0.3773... Train Metrics:0.8707... Val Loss:0.5109... Val Metrics:0.8250... Lr:0.00039...\n",
      "Time:0.996s... Epoch:21/100... Train Loss:0.3626... Train Metrics:0.8763... Val Loss:0.4938... Val Metrics:0.8324... Lr:0.00033...\n",
      "Time:0.922s... Epoch:22/100... Train Loss:0.3540... Train Metrics:0.8772... Val Loss:0.4874... Val Metrics:0.8334... Lr:0.00028...\n",
      "Time:0.925s... Epoch:23/100... Train Loss:0.3430... Train Metrics:0.8832... Val Loss:0.4837... Val Metrics:0.8357... Lr:0.00024...\n",
      "Time:0.919s... Epoch:24/100... Train Loss:0.3372... Train Metrics:0.8839... Val Loss:0.4722... Val Metrics:0.8381... Lr:0.00020...\n",
      "Time:1.113s... Epoch:25/100... Train Loss:0.3311... Train Metrics:0.8871... Val Loss:0.4649... Val Metrics:0.8419... Lr:0.00017...\n",
      "Time:0.905s... Epoch:26/100... Train Loss:0.3232... Train Metrics:0.8901... Val Loss:0.4567... Val Metrics:0.8447... Lr:0.00015...\n",
      "Time:0.912s... Epoch:27/100... Train Loss:0.3177... Train Metrics:0.8920... Val Loss:0.4528... Val Metrics:0.8470... Lr:0.00012...\n",
      "Time:1.010s... Epoch:28/100... Train Loss:0.3178... Train Metrics:0.8923... Val Loss:0.4501... Val Metrics:0.8472... Lr:0.00011...\n",
      "Time:0.908s... Epoch:29/100... Train Loss:0.3110... Train Metrics:0.8957... Val Loss:0.4478... Val Metrics:0.8493... Lr:0.00009...\n",
      "Time:0.910s... Epoch:30/100... Train Loss:0.3080... Train Metrics:0.8962... Val Loss:0.4472... Val Metrics:0.8493... Lr:0.00008...\n",
      "Time:0.902s... Epoch:31/100... Train Loss:0.3043... Train Metrics:0.8981... Val Loss:0.4439... Val Metrics:0.8504... Lr:0.00006...\n",
      "Time:1.108s... Epoch:32/100... Train Loss:0.3039... Train Metrics:0.8961... Val Loss:0.4430... Val Metrics:0.8509... Lr:0.00006...\n",
      "Time:0.915s... Epoch:33/100... Train Loss:0.2999... Train Metrics:0.8992... Val Loss:0.4425... Val Metrics:0.8512... Lr:0.00005...\n",
      "Time:0.924s... Epoch:34/100... Train Loss:0.2969... Train Metrics:0.9020... Val Loss:0.4403... Val Metrics:0.8517... Lr:0.00004...\n",
      "Time:0.977s... Epoch:35/100... Train Loss:0.2947... Train Metrics:0.9010... Val Loss:0.4383... Val Metrics:0.8522... Lr:0.00003...\n",
      "Time:0.922s... Epoch:36/100... Train Loss:0.2937... Train Metrics:0.9012... Val Loss:0.4382... Val Metrics:0.8526... Lr:0.00003...\n",
      "Time:0.924s... Epoch:37/100... Train Loss:0.2945... Train Metrics:0.9001... Val Loss:0.4375... Val Metrics:0.8533... Lr:0.00002...\n",
      "Time:0.917s... Epoch:38/100... Train Loss:0.2910... Train Metrics:0.9039... Val Loss:0.4374... Val Metrics:0.8528... Lr:0.00002...\n",
      "Time:1.090s... Epoch:39/100... Train Loss:0.2907... Train Metrics:0.9037... Val Loss:0.4381... Val Metrics:0.8523... Lr:0.00002...\n",
      "Time:0.890s... Epoch:40/100... Train Loss:0.2891... Train Metrics:0.9035... Val Loss:0.4366... Val Metrics:0.8541... Lr:0.00002...\n",
      "Time:0.898s... Epoch:41/100... Train Loss:0.2886... Train Metrics:0.9039... Val Loss:0.4365... Val Metrics:0.8537... Lr:0.00001...\n",
      "Time:0.919s... Epoch:42/100... Train Loss:0.2879... Train Metrics:0.9048... Val Loss:0.4364... Val Metrics:0.8543... Lr:0.00001...\n",
      "Time:0.906s... Epoch:43/100... Train Loss:0.2871... Train Metrics:0.9049... Val Loss:0.4360... Val Metrics:0.8538... Lr:0.00001...\n",
      "Time:0.899s... Epoch:44/100... Train Loss:0.2868... Train Metrics:0.9049... Val Loss:0.4358... Val Metrics:0.8542... Lr:0.00001...\n",
      "Time:0.891s... Epoch:45/100... Train Loss:0.2867... Train Metrics:0.9044... Val Loss:0.4353... Val Metrics:0.8548... Lr:0.00001...\n",
      "Time:1.084s... Epoch:46/100... Train Loss:0.2875... Train Metrics:0.9052... Val Loss:0.4356... Val Metrics:0.8548... Lr:0.00001...\n",
      "Time:0.889s... Epoch:47/100... Train Loss:0.2864... Train Metrics:0.9051... Val Loss:0.4354... Val Metrics:0.8544... Lr:0.00000...\n",
      "Time:0.897s... Epoch:48/100... Train Loss:0.2879... Train Metrics:0.9049... Val Loss:0.4365... Val Metrics:0.8546... Lr:0.00000...\n",
      "Time:0.890s... Epoch:49/100... Train Loss:0.2848... Train Metrics:0.9047... Val Loss:0.4356... Val Metrics:0.8543... Lr:0.00000...\n",
      "Time:1.026s... Epoch:50/100... Train Loss:0.2848... Train Metrics:0.9045... Val Loss:0.4359... Val Metrics:0.8548... Lr:0.00000...\n",
      "Time:0.905s... Epoch:51/100... Train Loss:0.2859... Train Metrics:0.9058... Val Loss:0.4361... Val Metrics:0.8546... Lr:0.00000...\n",
      "Time:0.906s... Epoch:52/100... Train Loss:0.2848... Train Metrics:0.9048... Val Loss:0.4353... Val Metrics:0.8548... Lr:0.00000...\n",
      "Time:1.045s... Epoch:53/100... Train Loss:0.2871... Train Metrics:0.9047... Val Loss:0.4357... Val Metrics:0.8546... Lr:0.00000...\n",
      "Time:0.922s... Epoch:54/100... Train Loss:0.2853... Train Metrics:0.9052... Val Loss:0.4358... Val Metrics:0.8543... Lr:0.00000...\n",
      "Time:0.932s... Epoch:55/100... Train Loss:0.2855... Train Metrics:0.9046... Val Loss:0.4359... Val Metrics:0.8541... Lr:0.00000...\n",
      "Time:0.923s... Epoch:56/100... Train Loss:0.2851... Train Metrics:0.9062... Val Loss:0.4359... Val Metrics:0.8536... Lr:0.00000...\n",
      "Time:1.125s... Epoch:57/100... Train Loss:0.2853... Train Metrics:0.9056... Val Loss:0.4364... Val Metrics:0.8541... Lr:0.00000...\n",
      "Time:0.925s... Epoch:58/100... Train Loss:0.2844... Train Metrics:0.9054... Val Loss:0.4359... Val Metrics:0.8546... Lr:0.00000...\n",
      "Time:0.931s... Epoch:59/100... Train Loss:0.2846... Train Metrics:0.9055... Val Loss:0.4354... Val Metrics:0.8546... Lr:0.00000...\n",
      "Time:1.010s... Epoch:60/100... Train Loss:0.2830... Train Metrics:0.9052... Val Loss:0.4365... Val Metrics:0.8536... Lr:0.00000...\n",
      "Time:0.934s... Epoch:61/100... Train Loss:0.2847... Train Metrics:0.9061... Val Loss:0.4356... Val Metrics:0.8546... Lr:0.00000...\n",
      "Time:0.934s... Epoch:62/100... Train Loss:0.2838... Train Metrics:0.9065... Val Loss:0.4359... Val Metrics:0.8545... Lr:0.00000...\n",
      "Time:0.909s... Epoch:63/100... Train Loss:0.2852... Train Metrics:0.9033... Val Loss:0.4353... Val Metrics:0.8551... Lr:0.00000...\n",
      "Time:1.117s... Epoch:64/100... Train Loss:0.2837... Train Metrics:0.9053... Val Loss:0.4355... Val Metrics:0.8544... Lr:0.00000...\n",
      "Time:0.909s... Epoch:65/100... Train Loss:0.2845... Train Metrics:0.9053... Val Loss:0.4355... Val Metrics:0.8548... Lr:0.00000...\n",
      "Time:0.916s... Epoch:66/100... Train Loss:0.2851... Train Metrics:0.9064... Val Loss:0.4357... Val Metrics:0.8542... Lr:0.00000...\n",
      "Time:0.936s... Epoch:67/100... Train Loss:0.2853... Train Metrics:0.9034... Val Loss:0.4356... Val Metrics:0.8543... Lr:0.00000...\n",
      "Time:0.934s... Epoch:68/100... Train Loss:0.2833... Train Metrics:0.9054... Val Loss:0.4362... Val Metrics:0.8541... Lr:0.00000...\n",
      "Time:0.913s... Epoch:69/100... Train Loss:0.2853... Train Metrics:0.9048... Val Loss:0.4360... Val Metrics:0.8540... Lr:0.00000...\n",
      "Time:0.906s... Epoch:70/100... Train Loss:0.2838... Train Metrics:0.9062... Val Loss:0.4349... Val Metrics:0.8552... Lr:0.00000...\n",
      "Time:1.093s... Epoch:71/100... Train Loss:0.2835... Train Metrics:0.9057... Val Loss:0.4352... Val Metrics:0.8548... Lr:0.00000...\n",
      "Time:0.905s... Epoch:72/100... Train Loss:0.2848... Train Metrics:0.9061... Val Loss:0.4365... Val Metrics:0.8540... Lr:0.00000...\n",
      "Time:0.920s... Epoch:73/100... Train Loss:0.2851... Train Metrics:0.9057... Val Loss:0.4360... Val Metrics:0.8545... Lr:0.00000...\n",
      "Time:0.911s... Epoch:74/100... Train Loss:0.2843... Train Metrics:0.9063... Val Loss:0.4353... Val Metrics:0.8551... Lr:0.00000...\n",
      "Time:1.055s... Epoch:75/100... Train Loss:0.2842... Train Metrics:0.9044... Val Loss:0.4351... Val Metrics:0.8547... Lr:0.00000...\n",
      "Time:0.901s... Epoch:76/100... Train Loss:0.2843... Train Metrics:0.9050... Val Loss:0.4355... Val Metrics:0.8546... Lr:0.00000...\n",
      "Time:0.910s... Epoch:77/100... Train Loss:0.2836... Train Metrics:0.9057... Val Loss:0.4358... Val Metrics:0.8543... Lr:0.00000...\n",
      "Time:0.994s... Epoch:78/100... Train Loss:0.2830... Train Metrics:0.9057... Val Loss:0.4356... Val Metrics:0.8546... Lr:0.00000...\n",
      "Time:0.909s... Epoch:79/100... Train Loss:0.2847... Train Metrics:0.9061... Val Loss:0.4355... Val Metrics:0.8546... Lr:0.00000...\n",
      "Time:0.916s... Epoch:80/100... Train Loss:0.2836... Train Metrics:0.9074... Val Loss:0.4357... Val Metrics:0.8545... Lr:0.00000...\n",
      "Time:0.905s... Epoch:81/100... Train Loss:0.2840... Train Metrics:0.9059... Val Loss:0.4351... Val Metrics:0.8551... Lr:0.00000...\n",
      "Time:1.095s... Epoch:82/100... Train Loss:0.2850... Train Metrics:0.9054... Val Loss:0.4360... Val Metrics:0.8544... Lr:0.00000...\n",
      "Time:0.903s... Epoch:83/100... Train Loss:0.2854... Train Metrics:0.9040... Val Loss:0.4357... Val Metrics:0.8543... Lr:0.00000...\n",
      "Time:0.910s... Epoch:84/100... Train Loss:0.2851... Train Metrics:0.9056... Val Loss:0.4355... Val Metrics:0.8547... Lr:0.00000...\n",
      "Time:0.901s... Epoch:85/100... Train Loss:0.2855... Train Metrics:0.9063... Val Loss:0.4358... Val Metrics:0.8546... Lr:0.00000...\n",
      "Time:1.051s... Epoch:86/100... Train Loss:0.2845... Train Metrics:0.9054... Val Loss:0.4355... Val Metrics:0.8544... Lr:0.00000...\n",
      "Time:0.897s... Epoch:87/100... Train Loss:0.2837... Train Metrics:0.9052... Val Loss:0.4355... Val Metrics:0.8542... Lr:0.00000...\n",
      "Time:0.905s... Epoch:88/100... Train Loss:0.2853... Train Metrics:0.9062... Val Loss:0.4361... Val Metrics:0.8544... Lr:0.00000...\n",
      "Time:0.923s... Epoch:89/100... Train Loss:0.2843... Train Metrics:0.9053... Val Loss:0.4350... Val Metrics:0.8553... Lr:0.00000...\n",
      "Time:0.919s... Epoch:90/100... Train Loss:0.2837... Train Metrics:0.9052... Val Loss:0.4354... Val Metrics:0.8541... Lr:0.00000...\n",
      "Time:0.929s... Epoch:91/100... Train Loss:0.2839... Train Metrics:0.9051... Val Loss:0.4361... Val Metrics:0.8544... Lr:0.00000...\n",
      "Time:0.916s... Epoch:92/100... Train Loss:0.2843... Train Metrics:0.9049... Val Loss:0.4355... Val Metrics:0.8546... Lr:0.00000...\n",
      "Time:0.987s... Epoch:93/100... Train Loss:0.2845... Train Metrics:0.9057... Val Loss:0.4351... Val Metrics:0.8546... Lr:0.00000...\n",
      "Time:0.916s... Epoch:94/100... Train Loss:0.2852... Train Metrics:0.9059... Val Loss:0.4354... Val Metrics:0.8546... Lr:0.00000...\n",
      "Time:0.913s... Epoch:95/100... Train Loss:0.2840... Train Metrics:0.9050... Val Loss:0.4356... Val Metrics:0.8541... Lr:0.00000...\n",
      "Time:0.991s... Epoch:96/100... Train Loss:0.2852... Train Metrics:0.9045... Val Loss:0.4354... Val Metrics:0.8548... Lr:0.00000...\n",
      "Time:0.919s... Epoch:97/100... Train Loss:0.2843... Train Metrics:0.9047... Val Loss:0.4359... Val Metrics:0.8546... Lr:0.00000...\n",
      "Time:0.926s... Epoch:98/100... Train Loss:0.2838... Train Metrics:0.9063... Val Loss:0.4355... Val Metrics:0.8541... Lr:0.00000...\n",
      "Time:0.930s... Epoch:99/100... Train Loss:0.2834... Train Metrics:0.9066... Val Loss:0.4360... Val Metrics:0.8543... Lr:0.00000...\n",
      "Time:0.925s... Epoch:100/100... Train Loss:0.2844... Train Metrics:0.9055... Val Loss:0.4354... Val Metrics:0.8552... Lr:0.00000...\n",
      "max_test_metrics:0.8553...\n"
     ]
    }
   ],
   "source": [
    "set_seed(2023)\n",
    "train_x = torch.Tensor(train_x)\n",
    "train_y = torch.Tensor(train_y)\n",
    "test_x = torch.Tensor(test_x)\n",
    "test_y = torch.Tensor(test_y)\n",
    "batch_size = 128\n",
    "train_loader = load_array((train_x, train_y), batch_size, False)\n",
    "test_loader = load_array((test_x, test_y), batch_size, False)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "net = FCN()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.85)\n",
    "num_epochs = 100\n",
    "\n",
    "# GPU训练\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if train_on_gpu:\n",
    "    print('Training in GPU')\n",
    "else:\n",
    "    print('No GPU available, training on CPU; consider making n_epochs vertargets small.')\n",
    "device = 'cuda' if train_on_gpu else 'cpu'\n",
    "\n",
    "train_loss, test_loss, train_metrics, test_metrics, best_model = \\\n",
    "    train(net, device, train_loader, test_loader, loss, num_epochs, optimizer, scheduler, sklearn.metrics.f1_score)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "def perturbation_rank(test_loader, best_model, DEVICE):\n",
    "    model = FCN()\n",
    "    model = model.to(DEVICE)\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    metrics = []\n",
    "    errors = []\n",
    "    names = ['P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8']\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i in range(8):\n",
    "            test_loss = []\n",
    "            test_metrics_batch = []\n",
    "            for idx, (sample, target) in enumerate(test_loader):\n",
    "                sample = sample.cpu().numpy()\n",
    "                np.random.shuffle(sample[:,:,i])\n",
    "                np.random.shuffle(sample[:,:,i+8])\n",
    "                sample, y = torch.from_numpy(sample).to(DEVICE).float(), target.to(DEVICE).long()\n",
    "                out = model(sample)\n",
    "                loss = nn.CrossEntropyLoss()(out, y)\n",
    "                test_loss.append(loss.item())\n",
    "                test_metrics_batch.append(sklearn.metrics.f1_score(y.cpu(), torch.argmax(out, dim=1).cpu(),\n",
    "                                                                   average='micro'))\n",
    "\n",
    "            test_loss_avg = np.mean(test_loss)\n",
    "            errors.append(test_loss_avg)\n",
    "\n",
    "            test_metrics_avg = np.mean(test_metrics_batch)\n",
    "            metrics.append(test_metrics_avg)\n",
    "            print(f'shuffle column {i+1}:')\n",
    "            print(f'Final Test Loss : {test_loss_avg:.4f}\\t | \\tFinal Test F1_Score : {test_metrics_avg:.4f}\\n')\n",
    "\n",
    "    max_error = np.max(errors)\n",
    "    importance = [e/max_error for e in errors]\n",
    "    data = {'name':names,'error':errors,'importance':importance}\n",
    "    result = pd.DataFrame(data,columns = ['name','error','importance'])\n",
    "\n",
    "    # max_metrics = np.max(metrics)\n",
    "    # importance = [e/max_metrics for e in metrics]\n",
    "    # importance = [1 - importan for importan in importance]\n",
    "    # data = {'name':names,'metrics':metrics,'importance':importance}\n",
    "    # result = pd.DataFrame(data,columns = ['name','metrics','importance'])\n",
    "\n",
    "    result.sort_values(by=['importance'],ascending=[0],inplace=True)\n",
    "    result.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate:\n",
      "\n",
      "Final Test Loss : 0.4350\t | \tFinal Test F1_Score : 85.5303\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(test_loader, best_model, device, loss)\n",
    "# result = perturbation_rank(test_loader, best_model, device)\n",
    "# print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}