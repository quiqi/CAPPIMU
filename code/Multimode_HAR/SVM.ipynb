{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "{'fall': 0,'run': 1,'walk': 2,'cycle': 3,'lay': 4,'squat': 5,'mop': 6,'drink': 7,'sweep': 8,'brushing_teeth': 9,\n",
    "    'cut': 10,'eat': 11,'folding_clothes': 12,'hang_out_clothes': 13,'ironing': 14,'open_door': 15,'open_fridge': 16,\n",
    "    'sit': 17,'stand': 18,'use_computer': 19,'wash_dish': 20,'wash_face': 21,'wash_window': 22,'watch_tv': 23,\n",
    "    'watering_flowers': 24,'write': 25,'wc': 26,'play_phone': 27,'switch': 28}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "first_label = 19\n",
    "last_label = 7\n",
    "\n",
    "import numpy as np\n",
    "train_x = np.load('/home/intelligence/Robin/Dataset/train_x.npy',allow_pickle=True)\n",
    "train_y = np.load('/home/intelligence/Robin/Dataset/train_y.npy',allow_pickle=True)\n",
    "test_x = np.load('/home/intelligence/Robin/Dataset/test_x.npy',allow_pickle=True)\n",
    "test_y = np.load('/home/intelligence/Robin/Dataset/test_y.npy',allow_pickle=True)\n",
    "\n",
    "valid_train_index = np.where((train_y == last_label) | (train_y == first_label))\n",
    "filter_label_train_x = train_x[valid_train_index]\n",
    "filter_label_train_y = train_y[valid_train_index]\n",
    "\n",
    "valid_test_index = np.where((test_y == last_label) | (test_y == first_label))\n",
    "filter_label_test_x = test_x[valid_test_index]\n",
    "filter_label_test_y = test_y[valid_test_index]\n",
    "\n",
    "print(\"x_train: {}, x_test: {}, y_train: {}, y_test: {}, \".format(filter_label_train_x.shape, filter_label_test_x.shape,filter_label_train_y.shape,filter_label_test_y.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 为防止标签越界，要将标签转化成0开始\n",
    "y_train_convert, y_test_convert = [], []\n",
    "for i in filter_label_train_y:\n",
    "    if i==first_label:\n",
    "        y_train_convert.append(0)\n",
    "    elif i==last_label:\n",
    "        y_train_convert.append(1)\n",
    "\n",
    "for i in filter_label_test_y:\n",
    "    if i==first_label:\n",
    "        y_test_convert.append(0)\n",
    "    elif i==last_label:\n",
    "        y_test_convert.append(1)\n",
    "y_train = np.array(y_train_convert)\n",
    "y_test = np.array(y_test_convert)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**SVM作二分类**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn import svm\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# classifier=svm.SVC()\n",
    "# parameters=[{'kernel': ['rbf'], 'gamma': [0.01, 0.001, 0.0001, 0.00001], 'C': [1, 10, 100, 1000]}]\n",
    "# model=GridSearchCV(classifier,parameters,n_jobs=-1,cv=4,verbose=1)\n",
    "# model.fit(train_x,train_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "# ypred=model.predict(test_x)\n",
    "# accuracy=accuracy_score(test_y,ypred)\n",
    "# print('Best Parameters: '+ str(model.best_params_))\n",
    "# print('Accuracy Score: '+ str(accuracy*100) + ' %')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**深度学习框架作二分类**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, n_channels=16, n_timesteps=350, n_classes=2, out_channels=128):\n",
    "        super(FCN, self).__init__()\n",
    "\n",
    "        self.conv_block1 = nn.Sequential(nn.Conv1d(n_channels, 32, kernel_size=8, stride=1, bias=False, padding=4),\n",
    "                                         nn.BatchNorm1d(32),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.MaxPool1d(kernel_size=2, stride=2, padding=1),\n",
    "                                         nn.Dropout(0.35))\n",
    "        self.conv_block2 = nn.Sequential(nn.Conv1d(32, 64, kernel_size=8, stride=1, bias=False, padding=4),\n",
    "                                         nn.BatchNorm1d(64),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.MaxPool1d(kernel_size=2, stride=2, padding=1))\n",
    "        self.conv_block3 = nn.Sequential(nn.Conv1d(64, out_channels, kernel_size=8, stride=1, bias=False, padding=4),\n",
    "                                         nn.BatchNorm1d(out_channels),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.MaxPool1d(kernel_size=2, stride=2, padding=1))\n",
    "\n",
    "        self.out_len = n_timesteps\n",
    "        for _ in range(3):\n",
    "            self.out_len = (self.out_len + 1) // 2 + 1\n",
    "\n",
    "        self.out_channels = out_channels  # 128\n",
    "        self.out_dim = self.out_len * self.out_channels\n",
    "\n",
    "        self.logits = nn.Linear(self.out_len * out_channels, n_classes)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        x_in = x_in.permute(0, 2, 1)\n",
    "        x = self.conv_block1(x_in)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "\n",
    "        x_flat = x.reshape(x.shape[0], -1)\n",
    "        logits = self.logits(x_flat)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class TPN(nn.Module):\n",
    "    def __init__(self, in_channels=16, n_classes=2):\n",
    "        super().__init__()\n",
    "        kernel_list = [24, 16, 8]\n",
    "        channels_size = [32, 64, 96]\n",
    "\n",
    "        self.out_dim = channels_size[2]\n",
    "        convNet_layers = [\n",
    "            ('conv1d_1', nn.Conv1d(in_channels, channels_size[0], kernel_size=(kernel_list[0],), stride=(1,))),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('dropout1', nn.Dropout(p=0.1)),\n",
    "            ('conv1d_2', nn.Conv1d(channels_size[0], channels_size[1], kernel_size=(kernel_list[1],), stride=(1,))),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('dropout2', nn.Dropout(p=0.1)),\n",
    "            ('conv1d_3', nn.Conv1d(channels_size[1], channels_size[2], kernel_size=(kernel_list[2],), stride=(1,))),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('dropout3', nn.Dropout(p=0.1)),\n",
    "            ('globalMaxPool', nn.AdaptiveMaxPool1d(output_size=1)),\n",
    "            ('flatten', nn.Flatten()),\n",
    "        ]\n",
    "        self.convNet = nn.Sequential(OrderedDict(convNet_layers))\n",
    "        self.classifier = nn.Linear(self.out_dim, n_classes)\n",
    "\n",
    "    def forward(self, x, data_format='channel_last'):\n",
    "        if data_format == 'channel_last':\n",
    "            x = x.permute(0, 2, 1)\n",
    "        x = self.convNet(x)\n",
    "        out = self.classifier(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DeepConvLSTM(nn.Module):\n",
    "    def __init__(self, n_channels=16, n_classes=2, conv_kernels=64, kernel_size=5, LSTM_units=128):\n",
    "        super(DeepConvLSTM, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, conv_kernels, (kernel_size, 1))\n",
    "        self.conv2 = nn.Conv2d(conv_kernels, conv_kernels, (kernel_size, 1))\n",
    "        self.conv3 = nn.Conv2d(conv_kernels, conv_kernels, (kernel_size, 1))\n",
    "        self.conv4 = nn.Conv2d(conv_kernels, conv_kernels, (kernel_size, 1))\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.lstm = nn.LSTM(n_channels * conv_kernels, LSTM_units, num_layers=1)\n",
    "\n",
    "        self.out_dim = LSTM_units\n",
    "\n",
    "        self.classifier = nn.Linear(LSTM_units, n_classes)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.lstm.flatten_parameters()\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.activation(self.conv1(x))\n",
    "        x = self.activation(self.conv2(x))\n",
    "        x = self.activation(self.conv3(x))\n",
    "        x = self.activation(self.conv4(x))\n",
    "\n",
    "        x = x.permute(2, 0, 3, 1)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x, h = self.lstm(x)\n",
    "        x = x[-1, :, :]\n",
    "\n",
    "        out = self.classifier(x)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from torch.utils import data\n",
    "import sklearn.metrics\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"构造一个pytorch数据构造器\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "def train(net, train_loader, test_loader, loss, num_epochs, optimizer, metrics):\n",
    "    \"\"\"训练模型\"\"\"\n",
    "    # GPU训练\n",
    "    train_on_gpu = torch.cuda.is_available()\n",
    "    # train_on_gpu = False\n",
    "    if(train_on_gpu):\n",
    "        print('Training in GPU')\n",
    "    else:\n",
    "        print('No GPU available, training on CPU; consider making n_epochs vertargets small.')\n",
    "    device = 'cuda' if train_on_gpu else 'cpu'\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "\n",
    "    # 声明需要保存的量\n",
    "    train_loss, test_loss, train_metrics, test_metrics, time_list = [], [], [], [], []\n",
    "    max_test_metrics = 0.0\n",
    "    # animator = d2l.Animator(xlabel='epoch', xlim=[1,num_epochs], ylim=[0,1],legend=['train loss', 'test loss', 'test accuracy'])\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss_batch, test_loss_batch, train_metrics_batch, test_metrics_batch = [], [], [], []\n",
    "        start = time.time()\n",
    "        # 训练\n",
    "        if isinstance(net, nn.Module):\n",
    "            net.train()  # 设置成训练模式，要计算梯度\n",
    "        for X, y in train_loader:\n",
    "            # 获取训练数据和标签\n",
    "            X, y = X.to(device).float(), y.to(device).long()\n",
    "            # 数据输入网络，前向传播\n",
    "            y_hat = net(X)\n",
    "            # 网络输出和标签输入损失函数，求得损失\n",
    "            l = loss(y_hat, y)\n",
    "            if isinstance(optimizer, torch.optim.Optimizer):\n",
    "                optimizer.zero_grad()  # 清零优化器，梯度清零\n",
    "                l.backward()  # 损失反向传播，计算梯度\n",
    "                optimizer.step()  # 更新优化器，更新参数\n",
    "            # 累加损失\n",
    "            train_loss_batch.append(l.item())\n",
    "            train_metrics_batch.append(metrics(y.cpu(), torch.argmax(y_hat, dim=1).cpu()))\n",
    "        # 累加训练损失\n",
    "        train_loss.append(np.mean(train_loss_batch))\n",
    "        train_metrics.append(np.mean(train_metrics_batch))\n",
    "        time_list.append(time.time() - start)\n",
    "        # 验证\n",
    "        if isinstance(net, nn.Module):  # 如果是用torch.nn实现的模型，就将它转换成评估模式\n",
    "            net.eval()  # 将模型设置为评估模式：不再计算梯度，只做forward pass\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y.long())\n",
    "            test_loss_batch.append(l.item())\n",
    "            test_metrics_batch.append(metrics(y.cpu(), torch.argmax(y_hat, dim=1).cpu()))\n",
    "        # 累加验证损失\n",
    "        test_loss.append(np.mean(test_loss_batch))\n",
    "        test_metrics.append(np.mean(test_metrics_batch))\n",
    "        # 打印信息\n",
    "        print(\"Time:{:.3f}s...\".format(time_list[-1]),\n",
    "              \"Epoch:{}/{}...\".format(epoch + 1, num_epochs),\n",
    "              \"Train Loss:{:.4f}...\".format(train_loss[-1]),\n",
    "              \"Train Metrics:{:.4f}...\".format(train_metrics[-1]),\n",
    "              \"Val Loss:{:.4f}...\".format(test_loss[-1]),\n",
    "              \"Val Metrics:{:.4f}...\".format(test_metrics[-1]),\n",
    "              \"Lr:{:.5f}...\".format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "        if max_test_metrics < test_metrics[-1] and epoch > num_epochs*0.7:\n",
    "            max_test_metrics = test_metrics[-1]\n",
    "\n",
    "#         animator.add(epoch+1, (train_loss[-1],test_loss[-1],test_metrics[-1]))\n",
    "    print(\"max_test_metrics:{:.4f}...\".format(max_test_metrics))\n",
    "    return train_loss, test_loss, train_metrics, test_metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "set_seed(2023)\n",
    "train_x = torch.Tensor(filter_label_train_x)\n",
    "train_y = torch.Tensor(y_train)\n",
    "test_x = torch.Tensor(filter_label_test_x)\n",
    "test_y = torch.Tensor(y_test)\n",
    "batch_size = 64\n",
    "train_loader = load_array((train_x, train_y), batch_size)\n",
    "test_loader = load_array((test_x, test_y), batch_size)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# net = DeepConvLSTM()\n",
    "net = FCN()\n",
    "# net = TPN()\n",
    "# # optimizer = torch.optim.SGD(net.parameters(), lr=0.03, weight_decay=1e-4)    # DeepConvLSTM\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-4)   # DeepConvLSTM\n",
    "num_epochs = 100\n",
    "train_loss, test_loss, train_metrics, test_metrics = \\\n",
    "    train(net, train_loader, test_loader, loss, num_epochs, optimizer, sklearn.metrics.accuracy_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}